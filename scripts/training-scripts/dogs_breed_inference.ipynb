{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dogs Image Research Inference\n",
    "Esse notebook foi feito para testar os modelos de Dog breed comparando a acuracia de cada um sob o mesmo dataset, servindo assim para comparar o resultado diante de diferentes arquiteturas porem do mesmo input size. Ao final do notebook o melhor modelo baseado na acuracia do dataset de teste eh convertido para CPU e o formato ONNX para facilitar a inferencia pelo OpenCV.\n",
    "\n",
    "by: Crystal Silva Campos <https://github.com/campos537>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 \n",
    "Importa as bibliotecas necessarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import os\n",
    "# Plot library\n",
    "import matplotlib.pyplot as plt\n",
    "# Metric generation library based on confusion matrix\n",
    "from pycm import *\n",
    "# Fast array processing library\n",
    "import numpy as np\n",
    "# Deep Learning framework\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "Aqui se escolhe o tamanho do batch de teste e o input size em que os modelos vao ser testados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = \"models/\"\n",
    "test_dir = \"test_cleaned\"\n",
    "\n",
    "\n",
    "input_size = (224,224)\n",
    "batch_size = 256\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "Aqui eh onde se carrega o dataset de teste e que eh feito o pre-processamento nas imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_dataset(input_size, test_dir, batch_size):\n",
    "    test_transforms = transforms.Compose([transforms.Resize(input_size),\n",
    "                                           transforms.ToTensor()\n",
    "                                          ])\n",
    "    test_data = datasets.ImageFolder(test_dir, transform = test_transforms)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle = True)\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "Nesse metodo o modelo eh testado e se compara com o ground-truth esperado assim medindo a acuracia e retornando a mesma juntamente com outras informacoes que vao ser passadas ao PyCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_network(loader, device_test, model):\n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        count = 0\n",
    "        actual_pred = []\n",
    "        ground_truth = []\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device_test), labels.to(device_test)\n",
    "            logps = model.forward(inputs)\n",
    "            # Calculate accuracy\n",
    "            ps = torch.exp(logps)\n",
    "            top_p, top_class = ps.topk(1, dim = 1)\n",
    "            equals = top_class == labels.view(*top_class.shape)\n",
    "            actual_pred.extend(top_class.squeeze().tolist())\n",
    "            ground_truth.extend(labels.view(*top_class.squeeze().shape).tolist())\n",
    "            accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "            count += 1\n",
    "    return (accuracy/len(loader)), actual_pred, ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "Nesse passo cada modelo eh testado, as pastas devem estar divididas da seguinte forma:\n",
    "\n",
    "models\n",
    "\n",
    "    resnet50-dogbreed\n",
    "    \n",
    "        epoch1.pth\n",
    "        \n",
    "        epoch2.pth\n",
    "        \n",
    "        ...\n",
    "    resnet34-dogbreed\n",
    "    \n",
    "        epoch1.pth\n",
    "        \n",
    "        epoch2.pth\n",
    "        \n",
    "        ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = load_test_dataset(input_size, test_dir, batch_size)\n",
    "\n",
    "cm = None\n",
    "best_accuracy_path = [\"-\",-999.0]\n",
    "best_model = None\n",
    "\n",
    "for model_type_path in os.listdir(models_path):\n",
    "    \n",
    "    models_list = os.listdir(models_path + '/' + model_type_path)\n",
    "    models_list.sort()\n",
    "    for model_path in models_list:\n",
    "        full_model_path = models_path + '/' + model_type_path + '/' + model_path\n",
    "        model_loaded = torch.load(full_model_path)\n",
    "        model_loaded.eval()\n",
    "        model_loaded.to(device)\n",
    "        print(\"TESTING --> \", model_path)\n",
    "        accuracy, actual_pred, ground_truth = test_network(test_loader,device,model_loaded)\n",
    "        print(\" ACCURACY --> \", accuracy , \"%\")\n",
    "        if accuracy > best_accuracy_path[1]:\n",
    "            best_accuracy_path[0] = model_path\n",
    "            best_accuracy_path[1] = accuracy\n",
    "            cm = ConfusionMatrix(actual_vector=ground_truth, predict_vector=actual_pred)\n",
    "            best_model = model_loaded\n",
    "    print(\"BEST MODEL INFO --> Type \", model_type_path, \" Model \", best_accuracy_path[0], \" Accuracy \", round(best_accuracy_path[1]*100,3), \"%ACC\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6\n",
    "Apos testar o modelo utilizando o PyCM a matrix de confusao com os resultados serao gerados e um reporte do melhor modelo sera criado. Tendo em vista a grande quantidade de classes isso dificulta a visualizacao de cada metrica individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm.save_html(model_type_path))\n",
    "cm.overall_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7\n",
    "Converte o modelo para onnx de modo a facilitar a inferencia com o OpenCV DNN e tambem a otimizacao para o Openvino caso seja feita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fc.add_module(\"prob_out\", nn.Softmax())\n",
    "best_model.eval()\n",
    "best_model.to(\"cpu\")\n",
    "output_onnx_name = best_accuracy_path[0][:-3] + \".onnx\"\n",
    "dummy_input = torch.randn(1, 3, input_size[0], input_size[1], device='cpu')\n",
    "torch.onnx.export(best_model, dummy_input, output_onnx_name , verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8\n",
    "Checa se a conversao para modelo ONNX ocorreu corretamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "onnx_model = onnx.load(output_onnx_name)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(onnx.helper.printable_graph(onnx_model.graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9\n",
    "Testa uma imagem de um shitzu(shih) com o modelo convertido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = test_loader.dataset.classes\n",
    "def process_output(out):\n",
    "        classId = out[0].argsort()[-5:][::-1]\n",
    "        confidence = [out[0][id_]*100 for id_ in classId]\n",
    "        return classId, confidence\n",
    "    \n",
    "img = cv2.imread(\"chewie1.png\")\n",
    "model = cv2.dnn.readNetFromONNX(output_onnx_name)\n",
    "blob =  cv2.dnn.blobFromImage(img, 1/255, (224,224), (), True, False)\n",
    "model.setInput(blob)\n",
    "out = model.forward()\n",
    "ids, proba = process_output(out)\n",
    "\n",
    "count = 0\n",
    "for breed in ids:\n",
    "    print(labels[breed],\" \", proba[count], \"%\")\n",
    "    count +=1\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.title('Shitzu')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dogs-env",
   "language": "python",
   "name": "dogs-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
