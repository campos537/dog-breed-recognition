{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dogs Image Research Inference\n",
    "Esse notebook foi feito para testar os modelos de Dog breed comparando a acuracia de cada um sob o mesmo dataset, servindo assim para comparar o resultado diante de diferentes arquiteturas porem do mesmo input size. Ao final do notebook o melhor modelo baseado na acuracia do dataset de teste eh convertido para CPU e o formato ONNX para facilitar a inferencia pelo OpenCV.\n",
    "\n",
    "by: Crystal Silva Campos <https://github.com/campos537>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 \n",
    "Importa as bibliotecas necessarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import os\n",
    "# Plot library\n",
    "import matplotlib.pyplot as plt\n",
    "# Metric generation library based on confusion matrix\n",
    "from pycm import *\n",
    "# Fast array processing library\n",
    "import numpy as np\n",
    "# Deep Learning framework\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "Aqui se escolhe o tamanho do batch de teste e o input size em que os modelos vao ser testados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = \"models/\"\n",
    "test_dir = \"test_cleaned\"\n",
    "\n",
    "\n",
    "input_size = (224,224)\n",
    "batch_size = 256\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "Aqui eh onde se carrega o dataset de teste e que eh feito o pre-processamento nas imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_dataset(input_size, test_dir, batch_size):\n",
    "    test_transforms = transforms.Compose([transforms.Resize(input_size),\n",
    "                                           transforms.ToTensor()\n",
    "                                          ])\n",
    "    test_data = datasets.ImageFolder(test_dir, transform = test_transforms)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle = True)\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "Nesse metodo o modelo eh testado e se compara com o ground-truth esperado assim medindo a acuracia e retornando a mesma juntamente com outras informacoes que vao ser passadas ao PyCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_network(loader, device_test, model):\n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        count = 0\n",
    "        actual_pred = []\n",
    "        ground_truth = []\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device_test), labels.to(device_test)\n",
    "            logps = model.forward(inputs)\n",
    "            # Calculate accuracy\n",
    "            ps = torch.exp(logps)\n",
    "            top_p, top_class = ps.topk(1, dim = 1)\n",
    "            equals = top_class == labels.view(*top_class.shape)\n",
    "            actual_pred.extend(top_class.squeeze().tolist())\n",
    "            ground_truth.extend(labels.view(*top_class.squeeze().shape).tolist())\n",
    "            accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "            count += 1\n",
    "    return (accuracy/len(loader)), actual_pred, ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "Nesse passo cada modelo eh testado, as pastas devem estar divididas da seguinte forma:\n",
    "\n",
    "models\n",
    "\n",
    "    resnet50-dogbreed\n",
    "    \n",
    "        epoch1.pth\n",
    "        \n",
    "        epoch2.pth\n",
    "        \n",
    "        ...\n",
    "    resnet34-dogbreed\n",
    "    \n",
    "        epoch1.pth\n",
    "        \n",
    "        epoch2.pth\n",
    "        \n",
    "        ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING -->  epoch_0_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/disk1/Repositorios/UnicoID/dogs-project/dogs-env/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ACCURACY -->  0.12197791039943695 %\n",
      "TESTING -->  epoch_10_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.881565979548863 %\n",
      "TESTING -->  epoch_11_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9217228123119899 %\n",
      "TESTING -->  epoch_12_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.880428169454847 %\n",
      "TESTING -->  epoch_13_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.8919657383646283 %\n",
      "TESTING -->  epoch_14_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.8938065256391253 %\n",
      "TESTING -->  epoch_15_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9081415278570992 %\n",
      "TESTING -->  epoch_16_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9150263837405613 %\n",
      "TESTING -->  epoch_17_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.902017627443586 %\n",
      "TESTING -->  epoch_18_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9033148799623761 %\n",
      "TESTING -->  epoch_19_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9057354501315525 %\n",
      "TESTING -->  epoch_1_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.3071732946804592 %\n",
      "TESTING -->  epoch_20_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9140914848872593 %\n",
      "TESTING -->  epoch_21_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9075762374060494 %\n",
      "TESTING -->  epoch_22_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9354707768985203 %\n",
      "TESTING -->  epoch_23_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9347533072744098 %\n",
      "TESTING -->  epoch_24_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9319486277444022 %\n",
      "TESTING -->  epoch_25_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9170628445489066 %\n",
      "TESTING -->  epoch_26_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9343619516917637 %\n",
      "TESTING -->  epoch_27_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9308325563158307 %\n",
      "TESTING -->  epoch_28_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9414424896240234 %\n",
      "TESTING -->  epoch_29_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9360360588346209 %\n",
      "TESTING -->  epoch_2_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.5838502986090524 %\n",
      "TESTING -->  epoch_30_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9287960955074855 %\n",
      "TESTING -->  epoch_31_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9412468075752258 %\n",
      "TESTING -->  epoch_32_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9384493827819824 %\n",
      "TESTING -->  epoch_33_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9423556327819824 %\n",
      "TESTING -->  epoch_34_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9464503100940159 %\n",
      "TESTING -->  epoch_35_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.936028812612806 %\n",
      "TESTING -->  epoch_36_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9293468849999564 %\n",
      "TESTING -->  epoch_37_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9419787781579154 %\n",
      "TESTING -->  epoch_38_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9592706390789577 %\n",
      "TESTING -->  epoch_39_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.950740669454847 %\n",
      "TESTING -->  epoch_3_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.6980157153947013 %\n",
      "TESTING -->  epoch_40_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9427397421428135 %\n",
      "TESTING -->  epoch_41_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9460879564285278 %\n",
      "TESTING -->  epoch_42_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9410583802631923 %\n",
      "TESTING -->  epoch_43_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.951653812612806 %\n",
      "TESTING -->  epoch_44_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9458850281579154 %\n",
      "TESTING -->  epoch_45_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9468199184962681 %\n",
      "TESTING -->  epoch_46_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.936028812612806 %\n",
      "TESTING -->  epoch_47_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9427252411842346 %\n",
      "TESTING -->  epoch_48_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9367825218609401 %\n",
      "TESTING -->  epoch_49_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.9254261340413775 %\n",
      "TESTING -->  epoch_4_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n",
      " ACCURACY -->  0.7787352119173322 %\n",
      "TESTING -->  epoch_50_inputsize_224_224_batchsize_256_name_dog_breed_resnet50_.pt\n"
     ]
    }
   ],
   "source": [
    "test_loader = load_test_dataset(input_size, test_dir, batch_size)\n",
    "\n",
    "cm = None\n",
    "best_accuracy_path = [\"-\",-999.0]\n",
    "best_model = None\n",
    "\n",
    "for model_type_path in os.listdir(models_path):\n",
    "    \n",
    "    models_list = os.listdir(models_path + '/' + model_type_path)\n",
    "    models_list.sort()\n",
    "    for model_path in models_list:\n",
    "        full_model_path = models_path + '/' + model_type_path + '/' + model_path\n",
    "        model_loaded = torch.load(full_model_path)\n",
    "        model_loaded.eval()\n",
    "        model_loaded.to(device)\n",
    "        print(\"TESTING --> \", model_path)\n",
    "        accuracy, actual_pred, ground_truth = test_network(test_loader,device,model_loaded)\n",
    "        print(\" ACCURACY --> \", accuracy , \"%\")\n",
    "        if accuracy > best_accuracy_path[1]:\n",
    "            best_accuracy_path[0] = model_path\n",
    "            best_accuracy_path[1] = accuracy\n",
    "            cm = ConfusionMatrix(actual_vector=ground_truth, predict_vector=actual_pred)\n",
    "            best_model = model_loaded\n",
    "    print(\"BEST MODEL INFO --> Type \", model_type_path, \" Model \", best_accuracy_path[0], \" Accuracy \", round(best_accuracy_path[1]*100,3), \"%ACC\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6\n",
    "Apos testar o modelo utilizando o PyCM a matrix de confusao com os resultados serao gerados e um reporte do melhor modelo sera criado. Tendo em vista a grande quantidade de classes isso dificulta a visualizacao de cada metrica individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm.save_html(model_type_path))\n",
    "cm.overall_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7\n",
    "Converte o modelo para onnx de modo a facilitar a inferencia com o OpenCV DNN e tambem a otimizacao para o Openvino caso seja feita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fc.add_module(\"prob_out\", nn.Softmax())\n",
    "best_model.eval()\n",
    "best_model.to(\"cpu\")\n",
    "output_onnx_name = best_accuracy_path[0][:-3] + \".onnx\"\n",
    "dummy_input = torch.randn(1, 3, input_size[0], input_size[1], device='cpu')\n",
    "torch.onnx.export(best_model, dummy_input, output_onnx_name , verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8\n",
    "Checa se a conversao para modelo ONNX ocorreu corretamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "onnx_model = onnx.load(output_onnx_name)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(onnx.helper.printable_graph(onnx_model.graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9\n",
    "Testa uma imagem de um shitzu(shih) com o modelo convertido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = test_loader.dataset.classes\n",
    "def process_output(out):\n",
    "        classId = out[0].argsort()[-5:][::-1]\n",
    "        confidence = [out[0][id_]*100 for id_ in classId]\n",
    "        return classId, confidence\n",
    "    \n",
    "img = cv2.imread(\"chewie1.png\")\n",
    "model = cv2.dnn.readNetFromONNX(output_onnx_name)\n",
    "blob =  cv2.dnn.blobFromImage(img, 1/255, (224,224), (), True, False)\n",
    "model.setInput(blob)\n",
    "out = model.forward()\n",
    "ids, proba = process_output(out)\n",
    "\n",
    "count = 0\n",
    "for breed in ids:\n",
    "    print(labels[breed],\" \", proba[count], \"%\")\n",
    "    count +=1\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.title('Shitzu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dogs-env",
   "language": "python",
   "name": "dogs-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
